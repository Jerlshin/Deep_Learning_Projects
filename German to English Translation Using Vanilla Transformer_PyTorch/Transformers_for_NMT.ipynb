{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **English to German translation using Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: portalocker in /home/jerlshin/anaconda3/envs/PyTorch_env/lib/python3.11/site-packages (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install spacy\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install altair\n",
    "!pip install GPUtil\n",
    "!pip install portalocker\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging \n",
    "import warnings\n",
    "from os.path import exists\n",
    "\n",
    "import GPUtil #interface for querying GPU information using NVIDIA GPUs\n",
    "import altair as alt # declarative statistical library, based on Vega-Lite visualizing Grammer. \n",
    "# Provides high level interface and visually applealing charts\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import time \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import spacy \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs:  [0]\n"
     ]
    }
   ],
   "source": [
    "gpus = GPUtil.getAvailable(order='first', limit=1, maxLoad=0.5, maxMemory=0.5, includeNan=False,\n",
    "                           excludeID=[], excludeUUID=[])\n",
    "\n",
    "print(\"Available GPUs: \", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs_dir = '/home/jerlshin/Documents/My_Work/__PROJECTS__/English to German Translation with tuned Transformers/results/logs'\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "log_file = os.path.join(logs_dir, 'project_logs.log')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_file,\n",
    "    filemode='a' # 'a' - append mode, 'w' - overwrite mode\n",
    ")\n",
    "\n",
    "\n",
    "def clear_logs():\n",
    "    with open(log_file, 'w'): # overwrite mode\n",
    "        pass\n",
    "\n",
    "clear_logs()\n",
    "\n",
    "logging.debug('Debug message')\n",
    "logging.info('Info message')\n",
    "logging.warning('Warning message')\n",
    "logging.error('Error message')\n",
    "logging.critical('Critical message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder architecture. \n",
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder, # process the src and generate a contextual representation of the input\n",
    "                 decoder, # generates the tgt based on encoder representation\n",
    "                 src_embed, # embedding layer for the src. tokens to continous vector representation\n",
    "                 tgt_embed,  # for tgt\n",
    "                 generator): # generator produces the final ouput probabilities.\n",
    "        \n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    # we can also use the same embed for both the source and target\n",
    "    def forward(self,\n",
    "                src, tgt,\n",
    "                src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        # encode and decode it \n",
    "        return self.decode(\n",
    "            self.encode(src, src_mask), # src for the decoder\n",
    "            src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # embed the src (token to vectors)\n",
    "        \"Masking to nask the padding added to the sequence to make it to equal lenght\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        # embed the tgt \n",
    "        # masking is added for training stability \n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Linear and softmax layer after the generation from the decoder\"\n",
    "\n",
    "    def __init__(self, d_model, # dim of the model, size of the output of the model \n",
    "                 vocab): # no of tokens\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # from torch.nn.functional import log_softmax\n",
    "        return log_softmax(self.proj(x), dim=-1) # conver to log prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    # simple Layer Norm, normalizing the activations of a layer \n",
    "    \n",
    "    def __init__(self, features,  # no of features in the input \n",
    "                 eps=1e-6): # const added to the denom for stability \n",
    "        \n",
    "        super(LayerNorm, self).__init__()\n",
    "\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculate the mean and standard deviation as per the formula \n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        \"\"\" Backbone\n",
    "        ( x - mean ) / std\n",
    "        \"\"\"\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder Clones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N): # the module of encoder and decoder with N stacks \n",
    "    \"Produce N identical layers.\" # nn.ModuleList()\n",
    "    return nn.ModuleList(\n",
    "        [copy.deepcopy(module) for _ in range(N)] # deepcopy() ensures that each layer in the stack is independent and does not share any parameters or grads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # encapsulate all the layers \n",
    "    \"Core encoder : stack of N layers\"\n",
    "    # composed of a stack of identical layers\n",
    "    def __init__(self, layer, N): # layer - individual encoder blocks \n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size) # applied to the output of the encoder stack \n",
    "\n",
    "    def forward(self, x, mask): # input, mask - to the input to handle padding \n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            \"\"\"output of the prev encoder block becomes the input to the next \"\"\"\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of each Sub-layer is \n",
    "\n",
    "LAYER_NORM (X + SUBLAYER(X))\n",
    "\n",
    "Where, SUBLAYER(X) is the function implemented by the sub-layer iteself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Layer Connection\n",
    "\n",
    "Implements a residual conection and layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        \n",
    "        self.norm = LayerNorm(size) # features --- size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer): # d_model = 512\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Smart Implementation... he he ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    # Single Layer in the Encoder of the Transformer. \n",
    "\n",
    "    def __init__(self, size, # dim of the model\n",
    "                 self_attn,  # self attn mechanism, MultiHead attn\n",
    "                 feed_forward,  # in the encoder\n",
    "                 dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward \n",
    "        \n",
    "        ## a list containing two instances of the SublayerConnection, res connection, dropout \n",
    "        \"Add & Norm\"\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2) # module and N \n",
    "\n",
    "        # model size \n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask): # input and mask \n",
    "        \"\"\"\n",
    "        Add and Normalization    (in the Transformer)\n",
    "        \"\"\"\n",
    "        # input and the sublayer \n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # input is passed to the attn in the sublayer block \n",
    "\n",
    "        return self.sublayer[1](x, self.feed_forward) # output of the encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Stacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, # each Decoder layer \n",
    "                 N): # no of repeats \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # clones of the layer \n",
    "        self.layers = clones(layer, N) # deepcopy() in the clones()\n",
    "        \n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers: # every decoder layer \n",
    "            x = layer(x, memory, src_mask, tgt_mask) \n",
    "            # output of prev, input of next \n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size,# model size 512 d_model  \n",
    "                 self_attn, # self attn, instance of MHA\n",
    "                 src_attn,  # encoder-decoder attn, instance of MHA \n",
    "                 feed_forward, # generic feed forward \n",
    "                 dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Masked MultiHead Attn -> MultiHead Attn -> Feed Forward  (3 add and norm)\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn \n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        # 3 Add and Norm\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "\n",
    "        m = memory # output of the encoder, context for the decoder \n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) # masked MHA\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) # ED MHA (Encoder -Decoder )\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the Self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    # such that, each position can only attent to previous positions and not o the positions that come after it in the sequence \n",
    "    attn_shape = (1, size, size) # upper triangular matrix \n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type( # diag = 1, main dian and eveyrthing below it is zeroed out \n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0 # zeroed can be attended. so the lower triangle will be 1's and upper traiangle will be 0's, so masking out the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled-dot Product Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "\n",
    "    # dim of the query vectors \n",
    "    d_k = query.size(-1) # size of the last dim\n",
    "    # raw scores \n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # scorresponding score is replaces with a large negative value \n",
    "    # softmax scores \n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    # as per formula\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):# no of attn heads\n",
    "        # h = 8\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        # This should be true, 512/8 = 64 \n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h # d_k - - dim of each head \n",
    "        self.h = h\n",
    "\n",
    "        # 4 Linear mapping from d_model to d_model. \n",
    "        \"\"\"Project Q, K, V and Final Linear Transformation (output Projection ) \"\"\"\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None # initialize attn to None \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # add new dim \n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        \"\"\"all the heads are processed in parallel\"\"\"\n",
    "        # Predict Q, K, V \n",
    "        query, key, value = [  # calculate the matrices # Transpose so, (n, h, len, d_k) to align with the multi head strucutre \n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batch_num, len of the input seq, h_num, d_k [size of each head ])\n",
    "            for lin, x in zip(self.linears, (query, key, value)) # linear layer and the input tensors (x)\n",
    "        ]\n",
    "        # 4th layer will be applied as a final linear transformation after the attention mechanism \n",
    "\n",
    "        # apply attention \n",
    "        x, self.attn = attention( # acording to the attention block, we take the one with value multiplied \n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # concatenation \n",
    "        x = (\n",
    "            x.transpose(1, 2) # (n, len, h, d_k)\n",
    "            .contiguous() # ensures that the tensors are stores ina contigous block of memory \n",
    "            .view(nbatches, -1, self.h * self.d_k) # 3 dim output \n",
    "        )\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x) # apply the last layer here \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Wise Feed Forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.w_1 = nn.Linear(d_model, d_ff) # dim of ff (d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model) # d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu())) # apply activation and then dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        \"\"\"Look Up table \"\"\" # each row --- embed vecotr of specific token \n",
    "        self.lut = nn.Embedding(vocab, d_model) # take the vocab, and convert to the size of the d_model\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000): # max len of the tokens \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model) # Position encoding \n",
    "        # each row - position ; each col - dim in embed space \n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # col vector 0 - ( max_len - 1 )\n",
    "        \n",
    "        # Follow the Formulation        \n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sine to even pos\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cosine to the odd pos\n",
    "        pe = pe.unsqueeze(0) # add batch dim to the pos enc\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False) # No need to train this , not trainable param \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab,  # size of the vocabulary of each \n",
    "    N=6, d_model=512, d_ff=2048, # dim of the feed forward network\n",
    "    h=8, dropout=0.1 # head and dropout \n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy # creates a deep copy function. \n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), # encoder \n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), # decder \n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), # src_embed\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), # tgt_embed \n",
    "        Generator(d_model, tgt_vocab), # generator \n",
    "    )\n",
    "\n",
    "    \"\"\" # Initialize parameters with Glorot / fan_avg. \"\"\"\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p) # initialize the parameters with the Glorot avg\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        # add dim to the mask\n",
    "        self.src_mask = (src != pad).unsqueeze(-2) # mask to indicate the padding position\n",
    "        # if the tgt is presetn\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1] # remove the last token\n",
    "            self.tgt_y = tgt[:, 1:] # exclued the first token\n",
    "            #  create a mask. this masking hides both the padding and the future words\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum() # counts the number of non-padded tokens in the target sequence\n",
    "\n",
    "    @staticmethod # indicates that the following method is a static method \n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Looop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # number of steps withing the current epoch \n",
    "    accum_step: int = 0  # of gradient accumulation steps. \n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(), # train state class \n",
    "):\n",
    "    # training a single epoch \n",
    "    start = time.time() # for each epoch \n",
    "\n",
    "    total_tokens = 0 # num of tokens processed \n",
    "    \n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_iter): # batch in each epoch \n",
    "        \n",
    "        # Forward Pass\n",
    "        out = model.forward( # get the predection \n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "\n",
    "        # Loss for the prediction. \n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward() # Backprop\n",
    "            train_state.step += 1 # count\n",
    "            train_state.samples += batch.src.shape[0] # total num of samples in the batch src\n",
    "            train_state.tokens += batch.ntokens # ntokens in the whole batch \n",
    "\n",
    "            # perform the optimization step based on the accumulation steps\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start # total time for one epoch \n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        \n",
    "        # empty the memory\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on WMT 2014 English-German dataset consisting of 4.5 Million sentence pairs. Sentence encoded using byte-pair encoding, which has source-target vocab of about 37000 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    To calculate a lr multiplier based on the current training step, model_size, scaling factor and warm-up parameter\n",
    "    \"\"\"\n",
    "    # default the step = 1 for LambdaLR function to avoid zero raising to negative power\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smooting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    \"\"\"\n",
    "    Regularization methods. Prevent the model from overfitting to the training data \n",
    "    by introducing a samll amount of uncertainity in the target labels \n",
    "    \"\"\"\n",
    "    def __init__(self, size, # num of classes in the classification task  \n",
    "                 padding_idx, # idex of the padding token, (2)\n",
    "                 smoothing=0.0): # smoothing param # hyperparameter\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "\n",
    "        \"\"\"KL Divergence Loss (Kullback-Leibler Divergence) criteriosn \"\"\"\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing # confidence value for smoothing \n",
    "        self.smoothing = smoothing # hyperparameter \n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        assert x.size(1) == self.size # logits match the size \n",
    "\n",
    "        # create a copy of the input loigit tensor 'x' -- distribution \n",
    "        true_dist = x.data.clone()\n",
    "        \"\"\"Fill with the constant / no of non-padding class\"\"\"\n",
    "        # fills the tensor with a smoothing value(constant). The smoothing is distributed uniformly across all class except the padding class\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2)) # (size - 2) # no of non padding classes: PAD, TARGET class (2)\n",
    "        \n",
    "        # apply confidene values to target classes. \n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        \"Idex tenor must have the same dim as the self tensor in the scatter_ operation\"\n",
    "        \n",
    "        # set prob of padding tokens to 0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "\n",
    "        # handle padding tokens in Target \n",
    "        mask = torch.nonzero(target.data == self.padding_idx) # masks of positiosns where the targe is the padding token\n",
    "        \"Get indices of non zero elements \"\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        \n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "----------------------------------------\n",
      "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 2., 0., 0., 0.],\n",
      "        [0., 0., 3., 4., 0.],\n",
      "        [0., 0., 0., 0., 5.]])\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fill_\n",
    "tensor = torch.ones(4, 5)\n",
    "print(tensor)\n",
    "fill = tensor.fill_(0)\n",
    "print(fill)\n",
    "\n",
    "print('-'*40)\n",
    "\n",
    "## scatter_\n",
    "src = torch.arange(1, 11, dtype=tensor.dtype).reshape(shape=(2, 5))\n",
    "print(src)\n",
    "index = torch.tensor([[0, 1, 2, 2, 3]])\n",
    "scatter = tensor.scatter_(dim=0, index=index, src=src)\n",
    "print(scatter)\n",
    "\n",
    "print('-'*40)\n",
    "\n",
    "# index_fill_\n",
    "tensor = torch.zeros(3, 3)\n",
    "indices = torch.tensor([0, 1])\n",
    "tensor.index_fill_(dim=0, index=indices, value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm): # allows instances of the class to be called as functions \n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), # to 2D tensor # generated \n",
    "                y.contiguous().view(-1) # target value \n",
    "            )\n",
    "            / norm # divide by the norm \n",
    "        )\n",
    "        return sloss.data * norm, sloss # with and without dividin by the norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len,\n",
    "                  start_symbol): # start symbl of the decoding \n",
    "    \"Greedy decoding for seq generation using a transformer based model\"\n",
    "\n",
    "    # encode the input \n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # init the output tensor \n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "\n",
    "    for i in range(max_len - 1): # decoding loop.\n",
    "        # generate each token of the sequence \n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        # prob of the next word\n",
    "        prob = model.generator(out[:, -1])\n",
    "\n",
    "        # word with max prob\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0] # scalar value \n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens_(data_iter, # iterator over the data\n",
    "                 tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    \n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens_(train + val, tokenize_de, index=1), # + test\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Note: Stupid, this test data can't be loaded in this, plan everything, debug properly\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens_(train + val, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "6379\n",
      "6291\n"
     ]
    }
   ],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **```Language Data```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 29001\n",
      "Valid size: 1015\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "sos_tok = '<sos>'\n",
    "eos_tok = '<eos>'\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(f'[{string.punctuation}\\n]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_de(text):\n",
    "    text = preprocessing_text(text)\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    text = preprocessing_text(text)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = torchtext.datasets.Multi30k(\n",
    "    root='.data', split=('train', 'valid', 'test'), language_pair=('de', 'en'))\n",
    "\n",
    "class TextDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, raw_data):\n",
    "        self.datasets = list(raw_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datasets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, trg = self.datasets[idx]\n",
    "        src = [sos_tok] + tokenize_de(src) + [eos_tok]\n",
    "        trg = [sos_tok] + tokenize_en(trg) + [eos_tok]\n",
    "        return src, trg\n",
    "\n",
    "train_datasets_dum = TextDatasets(train_data)\n",
    "valid_datasets_dum = TextDatasets(valid_data)\n",
    "\n",
    "\n",
    "print(\"Train size:\", len(train_datasets_dum))\n",
    "print(\"Valid size:\", len(valid_datasets_dum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# yield tokens\n",
    "def yield_tokens(data_iter: Iterable, \n",
    "                  language: str) -> List[str]:\n",
    "    \n",
    "    language_index = {SRC_LANGUAGE: 0,\n",
    "                      TGT_LANGUAGE: 1}\n",
    "    \n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=2,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "de_vocab = vocab_transform[SRC_LANGUAGE]  \n",
    "en_vocab = vocab_transform[TGT_LANGUAGE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8014, 6191)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_vocab_size = len(de_vocab)\n",
    "en_vocab_size = len(en_vocab)\n",
    "\n",
    "de_vocab_size, en_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual data samples into a single batch that can be processed efficiently during training\n",
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    \"For use with PyTorch's DataLoader\"\n",
    "    \"\"\"\n",
    "    It processes a batch of data, (source, target) seq and returns a collated batch suitable for training a seq2seq model\n",
    "    \"\"\"\n",
    "    # Define Special tokens (creating two tensors)\n",
    "    bs_id = torch.tensor([0], device=device)  # <s> token id    # Begining of Sentence\n",
    "    eos_id = torch.tensor([1], device=device)  # </s> token id    # End of Sentence\n",
    "\n",
    "    src_list, tgt_list = [], []\n",
    "    \n",
    "    # for each pair in batch \n",
    "    for (_src, _tgt) in batch:\n",
    "        # \n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)), # tokenizing \n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            dim=0, # along dim 0\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)), # tokenize \n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        src_list.append(\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src), # remaining tokens for reaching the max_len\n",
    "                ),\n",
    "                value=pad_id, # ID of the padding token\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "    is_distributed=False, # distributed training DDP, FSDP\n",
    "):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "        )\n",
    "\n",
    "    # iterators of the dataset \n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
    "        language_pair=(\"de\", \"en\")\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    convert the iterators to Map-Style Dataset \n",
    "    \"\"\"\n",
    "    ### TRAIN\n",
    "    train_iter_map = to_map_style_dataset(\n",
    "        train_iter\n",
    "    )  \n",
    "    train_sampler = (\n",
    "        DistributedSampler(train_iter_map) if is_distributed else None\n",
    "    )\n",
    "    \n",
    "    ### VALID \n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "    valid_sampler = (\n",
    "        DistributedSampler(valid_iter_map) if is_distributed else None\n",
    "    )\n",
    "\n",
    "    ## DATALOADER\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(valid_sampler is None),\n",
    "        sampler=valid_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EXAMPLES = True\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "\"\"\"As we don't need opt or scheduler for Validation, we give a dummy thing\"\"\"\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAIN WORKER\"\"\"\n",
    "def train_nmt_custom_model(\n",
    "    gpu, # device \n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "):\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 8,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "    \n",
    "    \"\"\"-> Make the Model \"\"\"\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "\n",
    "    is_main_process = True\n",
    "\n",
    "    \"\"\"-> Loss Function \"\"\"\n",
    "    criterion = LabelSmoothing( # Loss function\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu) # conver to the GPU\n",
    "\n",
    "    \"\"\"-> Data Loader \"\"\"\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu, # device\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        max_padding=config[\"max_padding\"],\n",
    "    )\n",
    "\n",
    "    \"\"\"-> Optimizer \"\"\"\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "\n",
    "    \"\"\"-> Learning Rate Scheduler\"\"\"\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Instance of the class TrainState()\n",
    "    train_state = TrainState()\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # train mode\n",
    "        model.train()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "\n",
    "        # Training with each epoch run\n",
    "        _, train_state = run_epoch(\n",
    "            data_iter=(Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model=model,\n",
    "            loss_compute=SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "\n",
    "        \"\"\"Validation \"\"\"\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache() # empty the cache\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "6379\n",
      "6291\n"
     ]
    }
   ],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"multi30k_model_final.pt\"\n",
    "\n",
    "if not exists(model_path):\n",
    "    train_nmt_custom_model(\n",
    "        gpu=0, # device \n",
    "        ngpus_per_node=1,\n",
    "        vocab_src=vocab_src,\n",
    "        vocab_tgt=vocab_tgt,\n",
    "        spacy_de=spacy_de,\n",
    "        spacy_en=spacy_en,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 15 12:43:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce MX450           Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   54C    P0              N/A / ERR! |   1765MiB /  2048MiB |     30%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1333      G   /usr/lib/xorg/Xorg                          153MiB |\n",
      "|    0   N/A  N/A      2367      G   /usr/bin/kwin_x11                            22MiB |\n",
      "|    0   N/A  N/A      2433      G   /usr/bin/plasmashell                         17MiB |\n",
      "|    0   N/A  N/A      2575      G   /usr/bin/latte-dock                           9MiB |\n",
      "|    0   N/A  N/A      3190      G   ...ling,SpareRendererForSitePerProcess       22MiB |\n",
      "|    0   N/A  N/A      5301      C   ...aconda3/envs/PyTorch_env/bin/python     1382MiB |\n",
      "|    0   N/A  N/A     10297      G   ...sion,SpareRendererForSitePerProcess       55MiB |\n",
      "|    0   N/A  N/A     10310      G   ...43,262144 --variations-seed-version       93MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import os\n",
    "\n",
    "def extract_code_cells(notebook_path, output_file):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n",
    "        notebook_content = notebook_file.read()\n",
    "\n",
    "    notebook = nbformat.reads(notebook_content, as_version=4)\n",
    "\n",
    "    code_cells = [cell for cell in notebook.cells if cell.cell_type == 'code']\n",
    "    code_lines = [cell.source for cell in code_cells]\n",
    "\n",
    "    code_content = '\\n\\n'.join(code_lines)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_python_file:\n",
    "        output_python_file.write(code_content)\n",
    "\n",
    "# Replace 'your_notebook.ipynb' with the actual notebook filename\n",
    "notebook_path = 'Transformers_for_NMT.ipynb'\n",
    "output_file = 'main.py'\n",
    "\n",
    "extract_code_cells(notebook_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
