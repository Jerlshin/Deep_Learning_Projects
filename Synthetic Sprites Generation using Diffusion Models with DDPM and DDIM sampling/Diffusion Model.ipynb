{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sprites Generation using Diffusion Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "\n",
    "from IPython.display import HTML\n",
    "from typing import Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from Model_utils import *\n",
    "from Diffusion_utils import *\n",
    "from Dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):\n",
    "        '''\n",
    "        n_feat: Number of channels is the number of features, higher value will allow to capture more intricate features\n",
    "        n_cfeat: Number of context feature; dim of the vector \n",
    "        '''\n",
    "        super(ContextUnet, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height # assuming h == w. this must be divisible by 4\n",
    "        \n",
    "        # initialize the initial convolutional layer, n_feat is the number of intermediate feature maps \n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "    \n",
    "        # initialize the down-sampling of the U-net with 2 levels. increase the channels \n",
    "        self.down1 = UnetDown(n_feat, n_feat)    # down1 # [10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2*n_feat)  # down2 # [10, 256, 4, 4]\n",
    "        # In Downsampling, no of channels increase while spatial dimensions decrease \n",
    "        # The increase in channels helps capture more complex features \n",
    "        self.to_vec = nn.Sequential(\n",
    "            nn.AvgPool2d((4)), \n",
    "            nn.GELU()     # applies the GELU activation function element-wise to the pooled features\n",
    "        )\n",
    "        \n",
    "        # Embed the timesteps and the context labels with fully-connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        \n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "        \n",
    "        # intit the up sampling path with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h // 4, self.h // 4),   # kernel_size and stride determines the h x w\n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "        ) \n",
    "        \n",
    "        # Up sampling path \n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        # as we have embedding in between, the channels matches\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps \n",
    "            nn.GroupNorm(8, n_feat), # normalize \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "        '''map to the same number of channels as input, as we will subtract this and give this as input again'''\n",
    "\n",
    "    \n",
    "    def forward(self, x, t, c=None):\n",
    "        '''\n",
    "        x : (batch, n_feat, h, w) : image  - n_feat is actually channels \n",
    "        t : (batch, n_cfeat) : time step\n",
    "        c : (batch, n_classes) : context label  \"\"\"We use categorical instead of natural embedding of context\"\"\"\n",
    "        '''\n",
    "        # context mask says which samples to block the contex on\n",
    "        # initial conv layer\n",
    "        x = self.init_conv(x)\n",
    "        # down\n",
    "        down1 = self.down1(x)   \n",
    "        down2 = self.down2(down1)\n",
    "        \n",
    "        # conver the feature maps to a vecotor and apply an activation \n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None: # c is the context_mask\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "        \n",
    "        # embed context and timstep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        \n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion parameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64   # hidden dim feature\n",
    "n_cfeat = 5   # context vector of categorically 5 element  vector\n",
    "height = 16  # 16 x 16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DDPM noise Schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0000, 5.3333, 5.6667, 6.0000, 6.3333, 6.6667, 7.0000, 7.3333, 7.6667,\n",
       "        8.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8 - 5) * torch.linspace(0, 1, 10) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0000, -4.3333, -4.6667, -5.0000, -5.3333, -5.6667, -6.0000, -6.3333,\n",
       "        -6.6667, -7.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_t_dummy = (8 - 5) * torch.linspace(0, 1, 10) + 5\n",
    "a_t_dummy = 1 - b_t_dummy\n",
    "a_t_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Constructing \"Denoising Diffusion Probaiblistic Model\" noise schedule'''\n",
    "# determines the strength or level of noise added in each timestep \n",
    "\n",
    "# beta values at t. (beta2 - beta1) : scales values to lie within the range beta1 to beta2 ; generates equally spaced values from 0 to 1\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1   # + beta1 shifts the values to start from beta1\n",
    "\n",
    "# alpha values at t ; complement of the a_t, In the diffusion models, the sum of alpha and beta values at each time step is 1 \n",
    "a_t = 1 - b_t\n",
    "\n",
    "# cummulative produce of a_t as eponential values\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "# find the cumulative sim of log along dim=0 and the .exp() exponentiates the cumulative sum, which effectively represents the cumulative product of a_t as exponential values \n",
    "ab_t[0] = 1 # initia value 1 ensuring the starting point for noise scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model'''\n",
    "\n",
    "nn_model = ContextUnet(\n",
    "    in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, \n",
    "    height=height\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89400, 5)\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels_data = np.load('sprite_labels_nc_1788_16x16.npy')\n",
    "\n",
    "print(labels_data.shape)\n",
    "print(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprite shape: (89400, 16, 16, 3)\n",
      "labels shape: (89400, 5)\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(sfilename=\"./sprites_1788_16x16.npy\", \n",
    "                        lfilename=\"./sprite_labels_nc_1788_16x16.npy\",\n",
    "                        transform=transform,\n",
    "                        null_context=False\n",
    "            )\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_input(x, t, noise):\n",
    "    # x has 4 dim, so add 3 extra dim to the ab_t\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None ,None ]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    \n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    for x, _ in pbar:   # x: images\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        \n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps)\n",
    "         \n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling without the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove the predicted noise from the actual noise and we add some extra noise back to aviod mode collapse\n",
    "'''\n",
    "# denoising process\n",
    "\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None: # adds noise\n",
    "        z = torch.randn_like(x)  # with the same shape as the input  \n",
    "    # get the sqrt() of beta values at given time step t from pre calculated noise schedule b_t\n",
    "    noise = b_t.sqrt()[t] * z # multiply to calculate the diffusion noise\n",
    "    # computing the denoised mean \n",
    "    scaling_factor = (1 - a_t[t]) / (1 - ab_t[t]).sqrt()\n",
    "    mean = (x - pred_noise * scaling_factor) \n",
    "    normalized_mean = mean / a_t[t].sqrt()\n",
    "    # adds the denoised mean to the diffusion noise \n",
    "    return normalized_mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextUnet(\n",
       "  (init_conv): ResidualConvBlock(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (down1): UnetDown(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (down2): UnetDown(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (to_vec): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    (1): GELU(approximate='none')\n",
       "  )\n",
       "  (timeembed1): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (timeembed2): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (contextembed1): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (contextembed2): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (up0): Sequential(\n",
       "    (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (up1): UnetUp(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): UnetUp(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights of model without the context \n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_trained.pth\", map_location=device))\n",
    "nn_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sampling using DDPM standard algorithm\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting process\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor. level of noise to be added \n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. \n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        \n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf() # clears the current figure \n",
    "samples, intermediate_ddpm = sample_ddpm(32) # samples to generate \n",
    "# 32 samples, 4 rows, \n",
    "animation_ddpm = plot_sample(intermediate_ddpm, 32, 4, save_dir, \"ani_run\", None, save=False)\n",
    "# converts animation into HTML format to display it in the output \n",
    "\n",
    "video_filename = \"Process_DDPM_noise_added.mp4\"\n",
    "animation_ddpm.save(video_filename, writer=\"ffmpeg\", fps=5)\n",
    "print(f\"Animation saved as {video_filename}\")\n",
    "\n",
    "HTML(animation_ddpm.to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling without Adding Extra Noise\n",
    "\n",
    "It will result in model collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Without adding the noise'''\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_incorrect(n_sample):\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        '''Adding the noise'''\n",
    "        z = 0\n",
    "\n",
    "        eps = nn_model(samples, t)    \n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i%20==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddpm_incorrect(32)\n",
    "animation_no_noise = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "\n",
    "\n",
    "video_filename = \"Process_DDPM_with_no_noise.mp4\"\n",
    "animation_no_noise.save(video_filename, writer=\"ffmpeg\", fps=5)\n",
    "print(f\"Animation saved as {video_filename}\")\n",
    "\n",
    "\n",
    "HTML(animation_no_noise.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextUnet(\n",
       "  (init_conv): ResidualConvBlock(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (down1): UnetDown(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (down2): UnetDown(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (to_vec): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    (1): GELU(approximate='none')\n",
       "  )\n",
       "  (timeembed1): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (timeembed2): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (contextembed1): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (contextembed2): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (up0): Sequential(\n",
       "    (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (up1): UnetUp(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): UnetUp(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_31.pth\"))\n",
    "nn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sample DDPM with Context'''\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_context(n_sample, context, save_rate=20):\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device) # (batch_size, channels, h, w)\n",
    "    \n",
    "    intermediate = []\n",
    "    \n",
    "    for i in range(timesteps, 0, -1): # travel back in time \n",
    "        print(f\"Sampling timestep {i:3d}\", end='\\r')\n",
    "        \n",
    "        # reshape the tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "        \n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "        \n",
    "        eps = nn_model(samples, t, c=context)        \n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        \n",
    "        if i % save_rate==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    \n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddpm_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "\n",
    "file_name = 'DDPM_sample_with_context.mp4'\n",
    "animation_ddpm_context.save(file_name, writer=\"ffmpeg\", fps=15)\n",
    "print(f\"Animation saved as {file_name}\")\n",
    "\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, ctx, nrow=2):\n",
    "    labels = [\"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\", \"human\"]\n",
    "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4, 3))\n",
    "    axs = axs.flatten()\n",
    "    for img, context, ax in zip(imgs, ctx.detach().cpu().numpy(), axs):\n",
    "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(img)\n",
    "        label = labels[np.argmax(context)]\n",
    "        ax.set_title(f\"{label}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],    # hero\n",
    "    [1,0,0,0,0],    # non-hero \n",
    "    [0,0,1,0,0],    # food\n",
    "    [0,0,0,1,0],    # spell\n",
    "    [0,0,0,0,1],    # side-facing\n",
    "    [0,0,0,1,1]     # human\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "context_dropdown = widgets.Dropdown(\n",
    "    options=[\"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\", \"human\"],\n",
    "    description='Select Context:',\n",
    ")\n",
    "# Function to show images based on selected context\n",
    "def show_images_for_context(context):\n",
    "    # Convert the selected context to a one-hot vector\n",
    "    contexts = {\n",
    "        \"hero\": [1, 0, 0, 0, 0],\n",
    "        \"non-hero\": [1, 0, 0, 0, 0],\n",
    "        \"food\": [0, 0, 1, 0, 0],\n",
    "        \"spell\": [0, 0, 0, 1, 0],\n",
    "        \"side-facing\": [0, 0, 0, 0, 1],\n",
    "        \"human\": [1, 0, 0, 1, 0]\n",
    "    }\n",
    "\n",
    "    # Convert the context to a PyTorch tensor\n",
    "    ctx = torch.tensor([contexts[context]]).float().to(device)\n",
    "\n",
    "    # Generate images based on the selected context\n",
    "    n_samples = 6  # Number of images to generate\n",
    "    samples, _ = sample_ddpm_context(n_samples, ctx)\n",
    "\n",
    "    # Determine the layout based on the number of samples\n",
    "    ncols = min(n_samples, 3)\n",
    "    nrows = -(-n_samples // ncols)  # Ceiling division\n",
    "\n",
    "    # Create subplots with specific number of columns and rows\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Show the images in the subplots\n",
    "    for i in range(n_samples):\n",
    "        img = (samples[i].squeeze().permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "        axs[i].set_title(f\"Image {i + 1}\", fontsize=10)\n",
    "        axs[i].title.set_position([0.5, -0.3])  # Adjust title position below the image\n",
    "\n",
    "    # Hide empty subplots if any\n",
    "    for j in range(n_samples, nrows * ncols):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Event handler for dropdown value change\n",
    "def dropdown_eventhandler(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        show_images_for_context(change['new'])\n",
    "\n",
    "# Link the dropdown widget to the event handler\n",
    "context_dropdown.observe(dropdown_eventhandler)\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(context_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_ddim(x, t, t_prev, pred_noise):\n",
    "    ab = ab_t[t]\n",
    "    ab_prev = ab_t[t_prev]\n",
    "    \n",
    "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
    "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
    "    \n",
    "    return x0_pred + dir_xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_trained.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "\n",
    "# we can sample quickly using DDIM\n",
    "@torch.no_grad()\n",
    "def sample_ddim(n_sample, n=20):\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddim(32, n=25)\n",
    "animation_ddim = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "\n",
    "HTML(animation_ddim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/context_model_31.pth\", map_location=device))\n",
    "nn_model.eval() \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ddim_context(n_sample, context, n=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ctx = F.one_hot(torch.randint(0, 5, (32,)), 5).to(device=device).float()\n",
    "samples, intermediate = sample_ddim_context(32, ctx)\n",
    "animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "\n",
    "\n",
    "video_filename = \"DDIM_with_context.mp4\"\n",
    "animation_ddpm_context.save(video_filename, writer=\"ffmpeg\", fps=5)\n",
    "print(f\"Animation saved as {video_filename}\")\n",
    "\n",
    "HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, ctx, nrow=2):\n",
    "    labels = [\"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\", \"human\"]\n",
    "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4, 3))\n",
    "    axs = axs.flatten()\n",
    "    i = 0\n",
    "    for img, context, ax in zip(imgs, ctx.detach().cpu().numpy(), axs):\n",
    "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(img)\n",
    "        label = labels[i]\n",
    "        ax.set_title(f\"{label}\")\n",
    "        i = i + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],    # hero\n",
    "    [1,0,0,0,0],    # non-hero \n",
    "    [0,0,1,0,0],    # food\n",
    "    [0,0,0,1,0],    # spell\n",
    "    [0,0,0,0,1],    # side-facing\n",
    "    [0,0,0,1,1]     # human\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddim_context(ctx.shape[0], ctx)\n",
    "show_images(samples, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
